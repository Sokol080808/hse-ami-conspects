\section{Дискретная теория вероятностей}

\subsection{Вероятность}

\begin{defn}{Вероятностное пространство}{}
    Вероятностное пространство --- \( (\Omega, P) \),
    где \( \Omega = \{ \omega_1, \omega_2, \ldots, \omega_n \} \) --- множество элементарных исходов,
    а \( P \) --- функция \( \Omega \to \RR \),
    обладающая свойствами:
    \begin{itemize}
        \item
            \( 0 \leq P(\omega_i) \leq 1 \)
        \item
            \( \sum\limits_{i = 1}^{n} P(\omega_i) = 1 \)
    \end{itemize}
\end{defn}

\begin{example}{}{}
    \begin{enumerate}
        \item
            Подбрасывание монетки: \( \Omega = \{ \text{О}, \text{Р} \} \), вероятности --- \( \frac{1}{2} \)
        \item
            Подбрасывание кубика: \( \Omega = \{ 1, 2, 3, 4, 5, 6 \} \), вероятности --- \( \frac{1}{6} \)
    \end{enumerate}
\end{example}

\begin{defn}{Событие}{}
    Событие \( A \) --- подмножество \( \Omega \)

    Вероятность события \( P(A) = \sum\limits_{\omega_i \in A} P(\omega_i) \)
\end{defn}

\begin{example}{}{}
    \begin{enumerate}[start=3]
        \item
            Подбрасывание нечестной монетки \( t \) раз

            Решка падает с вероятностью \( p \), орел с \( 1 - p \)

            Тогда \( \Omega = \{ 0, 1 \}^t \)

            \( P( \{ a_1, \ldots, a_t \} ) = p^{|a|} \cdot (1 - p)^{t - |a|} \)
        \item
            Равновероятный случай

            \( P(\omega_i) = \frac{1}{n} \)

            Тогда \( P(A) = \frac{|A|}{n} \)
    \end{enumerate}
\end{example}

\subsection{Дерево событий}

Не повезло, мне лень это рисовать в tikz.

Условно, если хотим как-то описать все события вида ``случайная перестановка из символов \( (1, 2, 3) \)''
можно просто сделать бор, написав на ребрах вероятности.

\subsection{Свойства вероятности}

\begin{enumerate}
    \item
        \( P(\varnothing) = 0, P(\Omega) = 1 \)
    \item
        \( P(A \sqcup B) = P(A) + P(B) \)
    \item
        \( P(\overline{A}) = 1 - P(A) \)
    \item
        \( A \subseteq B \Rightarrow P(A) \leq P(B) \)
    \item
        Формула включений-исключений

        \begin{thrm}{}{}
            Пусть \( (\Omega, P) \) --- вероятностное пространство, \( A_1, \ldots, A_n \) --- события.
            Тогда выполнено
            \[
                P(A_1 \cup \ldots \cup A_n)
                =
                \sum\limits_{k = 1}^n (-1)^{k + 1} \sum\limits_{1 \leq i_1 < \ldots < i_k \leq n} P(A_{i_1} \cap \ldots \cap A_{i_k})
            \]
        \end{thrm}

        Введем индикаторную функцию \( I_A : \Omega \to R \), \( I_A(x) = (x \in A) \).

        Отсюда следует несколько очевидных свойств:
        \begin{itemize}
            \item
                \( I_{A \cap B} = I_A \cdot I_B \)
            \item
                \( I_{\overline{A}} = 1 - I_A \)
            \item
                \( I_{A \cup B} = I_A + I_B - I_{A \cap B}  \)
        \end{itemize}

        \begin{gather*}
            I_{\overline{A_1 \cup \ldots \cup A_n}} = I_{\overline{A_1} \cap \ldots \cap \overline{A_n}} =
                I_{\overline{A_1}} \cdot \ldots \cdot I_{\overline{A_n}} = (1 - I_{A_1}) \ldots (1 - I_{A_n})
            \\
            \Downarrow
            \\
            I_{A_1 \cup \ldots \cup A_n} = 1 - (1 - I_{A_1}) \ldots (1 - I_{A_n})
        \end{gather*}

        Раскрыв скобочки в последнем равенстве получим искомое.
\end{enumerate}

\begin{example}{Задача о беспорядках}{}
    \( \Omega = S_n \), перестановки равновероятны.

    \( \sigma \in S_n \) --- беспорядок, если \( \forall i \ \sigma(i) \neq i \)

    Необходимо посчитать \( P(\text{\( \sigma \) --- беспорядок}) \)
\end{example}

\( Y_i = \{ \sigma \in S_n \: \vline \: \sigma(i) = i \} \)

\( P(\text{\( \sigma \) --- не беспорядок}) = P(Y_1 \cup \ldots \cup Y_n) \)

\begin{gather*}
    P(Y_{i_1} \cap \ldots \cap Y_{i_k}) = \frac{(n - k)!}{n!}
    \\
    \Downarrow
    \\
    P(Y_1 \cup \ldots \cup Y_n) = \sum\limits_{k = 1}^{n} (-1)^{k + 1} C_n^k \frac{(n - k)!}{n!}
    =
    \sum\limits_{k = 1}^{n} \frac{(-1)^{k + 1}}{k!}
    \\
    \Downarrow
    \\
    P(\text{\( \sigma \) --- беспорядок})
    =
    1 - \sum\limits_{k = 1}^{n} \frac{(-1)^{k + 1}}{k!}
    =
    \sum\limits_{k = 0}^{n} \frac{(-1)^{k}}{k!}
\end{gather*}

При большом \( n \) такая штука стремится к \( \displaystyle \frac{1}{e} \)

\subsection{Условная вероятность}

\( P(A | B) \) --- вероятность события \( A \) при условии \( B \)

\[
    P(A | B) = \frac{P(A \cap B)}{P(B)}
\]

\begin{example}{}{}
    Есть коробки, пронумерованные от \( 1 \) до \( 4 \), и один шарик.

    Сначала равновероятно выбирается коробка, а потом подбрасывается монетка,
    и с вероятностью \( \frac{1}{2} \) шарик кладется в выбранную коробку.

    Ведущий показал, что в первых трех коробках нет шара,
    какова вероятность, что он в четвертой?
\end{example}

\( A \) --- шарик в четвертой коробке, \( B \) --- шарик не в первых трех коробках.

\( P(B) = \frac{5}{8} \) и \( P(A \cap B) = \frac{1}{8} \)

Получаем, что ответ равен \( \frac{1}{5} \)

\subsection{Формулы полной вероятности и Байеса}

\begin{thrm}{Формула полной вероятности}{}
    Пусть \( (\Omega, P) \) --- вероятностное пространство и \( \Omega = \bigsqcup\limits_{i = 1}^m A_i \),
    причем \( P(A_i) > 0 \)

    Тогда \( P(B) = \sum\limits_{i = 1}^{n} P(B | A_i) \cdot P(A_i) \)
\end{thrm}

\[
    \sum\limits_{i = 1}^{n} P(B | A_i) \cdot P(A_i)
    =
    \sum\limits_{i = 1}^{n} P(A_i \cap B)
    =
    P(B \cap (A_1 \cup \ldots \cup A_n))
    =
    P(B)
\]

\begin{thrm}{Формула Байеса}{}
    Пусть \( (\Omega, P) \) --- вероятностное пространство.

    Есть события \( A, B \) ненулевой вероятности.

    Тогда \( \displaystyle P(B | A)= \frac{P(A | B) \cdot P(B)}{P(A)} \)
\end{thrm}

\begin{gather*}
    P(B | A) \cdot P(A) = P(A \cap B) = P(A | B) \cdot P(B)
    \\
    \Downarrow
    \\
    P(B | A)= \frac{P(A | B) \cdot P(B)}{P(A)}
\end{gather*}

\begin{example}{}{}
    Болеет \( 1\% \) населения.

    Тест на болезнь ошибается в \( 1\% \) случаев.
\end{example}

Посчитаем \( P(\text{человек болен (A) | тест положительный (B)}) \)

\begin{gather*}
    P(A | B)
    =
    \frac{P(B | A) \cdot P(A)}{P(B)}
    =
    \frac{99\% \cdot 1\%}{P(B | A) \cdot P(A) + P(B | \overline{A}) \cdot P(\overline{A})}
    =
    \\
    =
    \frac{99\% \cdot 1\%}{99\% \cdot 1\% + 1\% \cdot 99\%}
    =
    0.5
\end{gather*}

\subsection{Независимые события}

Пусть \( (\Omega, P) \) --- вероятностное пространство.

\begin{defn}{}{}
    \begin{itemize}
        \item
            \( A \) и \( B \) --- независимые события, если \( P(A \cap B) = P(A) \cdot P(B) \)
        \item
            \( A_1, \ldots, A_m \) --- независимые в совокупности,
            если для любого подмножества \( A_{i_1}, \ldots, A_{i_k} \)
            выполнено
            \[
                P(A_{i_1} \cap \ldots \cap A_{i_k}) = P(A_{i_1}) \cdot \ldots \cdot P(A_{i_k})
            \]
    \end{itemize}
\end{defn}

Приведем пример, показывающий недостаточность попарной независимости для независимости в совокупности:
\begin{example}{}{}
    Подбросим монетку дважды.

    \( A_1 \) --- в первом броске выпал орел

    \( A_2 \) --- во втором броске выпал орел

    \( B \) --- выпал ровно один орел

    Нетрудно убедиться, что события попарно независимы, но при этом
    \[
        P(A_1 \cap A_2 \cap B) = 0 \neq \frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{2}
    \]
\end{example}

\begin{exercise}{}{}
    Докажите, что при подбрасывании монетке события вида \( A_i = (\text{в \( i \)-м броске выпал орел}) \) независимы в совокупности.
\end{exercise}

\subsection{Математическое ожидание}

\( (\Omega, P) \) --- вероятностное пространство.

\begin{defn}{}{}
    Случайная величина --- это \( f : \Omega \to \RR \)
\end{defn}

\begin{defn}{}{}
    Математическое ожидание \( f \) --- это \( E(f) = p_1 f(\omega_1) + \ldots + p_n f(\omega_n) = \sum\limits_{i = 1}^n p_i f(\omega_i) \)
\end{defn}

\begin{example}{}{}
    Математическое ожидание выпавшего на игральном кубике числа:
    \[
        \frac{1}{6} \cdot (1 + 2 + 3 + 4 + 5 + 6) = \frac{21}{6} = 3.5
    \]
\end{example}

Свойства математического ожидания:
\begin{itemize}
    \item
        \( f = const = c \) --- константная случайная величина

        \( E(f) = c \)

        \newpage
    \item
        \( A \subseteq \Omega \) --- событие,
        \(
            I_A (\omega) = \begin{cases}
                1, \: \omega \in A
                \\
                0, \: \omega \notin A
            \end{cases}
        \)

        \( E(I_A) = P(A) \)
    \item
        \(
            \displaystyle
            E(f) = \sum\limits_{x \in range(f)} x \cdot P(f = x)
        \)
\end{itemize}

\begin{lemma}{Линейность математического ожидания}{}
    Пусть \( (\Omega, P) \) --- вероятность пространство, \( f, g \) --- случайные величины.

    Тогда:
    \begin{itemize}
        \item
            \( E(f + g) = E(f) + E(g) \)
        \item
            \( E(c \cdot f) = c \cdot E(f) \)
    \end{itemize}
\end{lemma}

Доказательство тривиально (прямая проверка по формуле)

\begin{example}{Задача о днях рождения}{}
    Есть \( n = 28 \) людей, посчитать математическое ожидание числа пар людей,
    у которых даты совпали.
\end{example}

Считаем, что \( \Omega = \{ 1, 2, \ldots, 365 \}^n \), все варианты равновероятны.

Пусть \( f \) --- количество пар людей, родившихся в один день года.

\begin{exercise}{}{}
    Если \( \Omega \) не равновероятна, то \( E(f) \) может лишь увеличиться.
\end{exercise}

\( f = \sum\limits_{i < j} g_{ij} \), где
\(
    g_{ij} (\omega) = \begin{cases}
        1, \text{\( i, j \) родились в один день}
        \\
        0, \text{иначе}
    \end{cases}
\)

Понятно, что \( E(g_{ij}) = \frac{1}{365} \)

Тогда математическое ожидание \( f \):
\[
    E(f) = E \left( \sum\limits_{i < j} g_{ij} \right)
    =
    \sum\limits_{i < j} E(g_{ij})
    =
    \frac{n \cdot (n - 1)}{2} \cdot \frac{1}{365}
    =
    \frac{28 \cdot 27}{2 \cdot 365} = \frac{378}{365} > 1
\]

\begin{thrm}{}{}
    Пусть \( f : \Omega \to \RR_{\geq 0} \) --- неотрицательная случайная величина, \( \alpha > 0 \).

    Тогда \( P(f \geq \alpha) \leq \frac{E(f)}{\alpha} \)
\end{thrm}

Пусть \( f(\omega_i) = a_i \)
\[
    E(f) = p_1 a_1 + p_2 a_2 + \ldots + p_n a_n
\]

Заменим все числа \( \geq \alpha \) на \( \alpha \), а все меньшие --- на \( 0 \).
Тогда мы только уменьшим сумму, но в то же время получим оценку:
\[
    E(f) \geq P(f \geq \alpha) \cdot \alpha
\]

А именно это мы и хотели доказать.

\begin{exercise}{}{}
    Приведите контрпример в случае произвольной (то есть, не неотрицательной) случайной величины.
\end{exercise}

\begin{example}{}{}
    Имеется вероятностный алгоритм \( I \)
    \begin{itemize}
        \item
            Всегда работает правильно
        \item
            Иногда работает долго
        \item
            Среднее время работы \( T = O(n^2) \)
    \end{itemize}

    Хотим: алгоритм \( II \)
    \begin{itemize}
        \item
            Всегда работает за \( O(n^2) \)
        \item
            Иногда работает неправильно (\( \leq 0.01\% \) случаев)
    \end{itemize}
\end{example}

План: запустим алгоритм \( I \) \( 10000 \cdot T \) шагов,
после чего обрываем алгоритм. Если мы успели получить ответ,
то будем считать его результатом работы \( II \),
иначе выдадим какой-нибудь мусор.

Рассмотрим \( f \) --- время работы \( I \):
\[
    P(f \geq 10000T) \leq \frac{E(f)}{10000T} = \frac{T}{10000T} = \frac{1}{10000}
\]

\subsection{Дисперсия}

\begin{defn}{}{}
    Дисперсия случайной величины \( f : \Omega \to \RR \) --- это \( D(f) = E \left( (f - E(f))^2 \right) \)
\end{defn}

\begin{thrm}{}{}
    \[
        D(f) = E(f^2) - E(f)^2
    \]
\end{thrm}

\begin{gather*}
    D(f)
    =
    E \left( (f - E(f))^2 \right)
    =
    E ( f^2 - 2 f \cdot E(f) + (E(f))^2 )
    =
    \\
    =
    E(f^2) - 2 E (f \cdot E(f)) + E( E(f)^2 )
    =
    \\
    =
    E(f^2) - 2 E(f)^2 + E(f)^2
    =
    E(f^2) - E(f)^2
\end{gather*}

\begin{thrm}{Неравенство Чебышева}{}
    Пусть \( f \) --- случайная величина, \( \alpha > 0 \)

    Тогда \( P(|f - E(f)| \geq \alpha) \leq \frac{D(f)}{\alpha^2} \)
\end{thrm}

Пусть \( g = (f - E(f))^2 \geq 0 \)

Тогда
\(
    \displaystyle
    P(|f - E(f)| \geq \alpha) = P(g \geq \alpha^2) \leq \frac{E(g)}{\alpha^2} = \frac{D(f)}{\alpha^2}
\)

\subsection{Независимые случайные величины}

\begin{defn}{}{}
    Случайные величины \( f, g \) называются независимыми,
    если \( \forall x, y \in \RR \) выполнено \( P(f = x \cap g = y) = P(f = x) \cdot P(g = y) \)

    Случайные величины \( f_1, \ldots, f_k \) независимы в совокупности,
    если \( \forall x_1, \ldots, x_k \in \RR \) события
    ``\( f_1 = x_1 \)'', \ldots, ``\( f_k = x_k \)'' --- независимые в совокупности.
\end{defn}

\begin{thrm}{}{}
    Пусть \( f, g \) --- независимые случайные величины, тогда \( E(fg) = E(f) E(g) \)
\end{thrm}

\begin{gather*}
    E(f) = \sum\limits_{x \in \RR} x \cdot P(f = x), E(g) = \sum\limits_{y \in \RR} y \cdot P(g = y)
    \\
    E(f) E(g)
        = \left( \sum\limits_{x \in \RR} x \cdot P(f = x) \right)
        \left( \sum\limits_{y \in \RR} y P(g = y) \right)
    =
    \\
    =
    \sum\limits_{x, y \in \RR} xy \cdot P(f = x) \cdot P(g = y) = \sum\limits_{x, y \in \RR} xy \cdot P(f = x, \: g = y)
    =
    E(fg)
\end{gather*}

\begin{thrm}{}{} 
    Пусть \( f, g \) --- независимые случайные величины, тогда \( D(f + g) = D(f) + D(g) \)
\end{thrm}

\begin{gather*}
    D(f + g)
    =
    E(f + g)^2 - (E(f + g))^2
    =
    E(f^2 + 2fg + g^2) - (E(f) + E(g))^2
    =
    \\
    =
    E(f^2) + 2E(fg) + E(g^2) - (E(f))^2 - 2 E(f) E(g) - (E(g))^2
    =
    \\
    =
    (E(f^2) - (E(f))^2) + (E(g^2) - (E(g))^2)
    =
    D(f) + D(g)
\end{gather*}

\begin{exercise}{}{}
    Докажите эти факты для \( n \) независимых в совокупности величин
\end{exercise}

\subsection{Оценки биномиальных коэффициентов}

Подбросим монету \( n \) раз (\( n \vdots 2 \)), посчитаем вероятность того, что выпала ровно половина орлов:
\[
    P \left( \text{выпало \( \frac{n}{2} \)} \right) = \frac{C_n^{n / 2}}{2^n}
\]

\begin{thrm}{Формула Стирлинга}{}
    \[
        n! \sim \sqrt{2 \pi n} \left( \frac{n}{e} \right)^n
    \]
\end{thrm}

\[
    \frac{C_n^{n / 2}}{2^n}
    =
    \frac{n!}{ (n / 2)! \cdot (n / 2)! \cdot 2^n}
    \sim
    \frac{\sqrt{2 \pi n} \left( \frac{n}{e} \right)^n }{2^n \cdot \pi n \cdot \left( \frac{n}{2e} \right)^n }
    =
    \frac{\sqrt{2 \pi n}}{\pi n}
    =
    \frac{\sqrt{2}}{\sqrt{\pi n}}
\]

\begin{thrm}{Оценка биномиальных коэффициентов}{}
    \[
        \left( \frac{n}{k} \right)^k \leq C_n^k \leq \left( \frac{ne}{k} \right)^k
    \]
\end{thrm}

\[
    C_n^k
    =
    \frac{n \cdot (n - 1) \cdot \ldots \cdot (n - k + 1)}{k \cdot (k - 1) \cdot \ldots \cdot 1}
    \geq
    \left( \frac{n}{k}\right)^k
\]

Вспомним факт \( \forall x \in \RR \quad e^x \geq 1 + x \).

Для оценки сверху покажем, что
\(
    \displaystyle
    \sum \limits_{i = 0}^{k} C_n^i \leq \left( \frac{ne}{k} \right)^k
\)

Рассмотрим \( t \in (0, 1) \):
\[
    \sum \limits_{i = 0}^{k} C_n^i
    \leq
    \sum \limits_{i = 0}^{k} C_n^i \cdot \frac{t^i}{t^k}
    =
    \frac{1}{t^k} \sum \limits_{i = 0}^{k} C_n^i \cdot t^i
    \leq
    \frac{1}{t^k} (1 + t)^n
\]

Возьмем \( t = \frac{k}{n} \):
\[
    \sum \limits_{i = 0}^{k} C_n^i
    \leq
    \frac{n^k}{k^k} \cdot \left( 1 + \frac{k}{n} \right)^n
    =
    \left( \frac{n}{k} \right)^k \cdot \left( 1 + \frac{k}{n} \right)^{\frac{n}{k} \cdot k}
    \leq
    \left( \frac{n}{k} \right)^k \cdot e^{k}
    =
    \left( \frac{ne}{k} \right)^k
\]

\subsection{Вероятностные методы}

\begin{lemma}{Вероятностный метод}{}
    Пусть \( f : \Omega \to \RR \) --- случайная величина, \( E(f) = c \).

    Тогда:
    \begin{enumerate}
        \item
            \( \exists \omega_{ \min } \in \Omega : f ( \omega_{ \min } ) \leq c \)
        \item
            \( \exists \omega_{ \max } \in \Omega : f ( \omega_{ \max } ) \geq c \)
    \end{enumerate}
\end{lemma}

Доказательство: тривиально

\begin{defn}{}{}
    \( G = (V, E) \) --- простой неориентированный граф.

    Разрез в \( G \) --- это \( S \subseteq V \).

    Величина разреза --- это \( | E(S, V \setminus S) | \)
\end{defn}

\begin{thrm}{}{}
    Пусть \( G = (V, E) \) --- простой неориентированный граф,
    тогда существует разрез величины хотя бы \( \frac{|E|}{2} \).
\end{thrm}

\( \Omega = 2^V, \quad | \Omega | = 2^n \), каждый разрез равновероятен.

За случайную величину примем величину разреза.

Очевидно, что \( E(X) = \frac{|E|}{2} \) (доказывается через индикаторы).

\begin{thrm}{}{}
    Пусть в графе \( G = (V, E) \) \( 2n \) вершин, тогда сушествует разрез
    величины хотя бы \( \frac{|E| \cdot n}{2n - 1} \).
\end{thrm}

Давайте выбирать все разрезы размера \( n \) равновероятно.

\[
    E(I_e)
    =
    \frac{2 \cdot C_{2n - 2}^{n - 1}}{C_{2n}^n}
    =
    \frac{2 \cdot (2n - 2)! \cdot n! \cdot n!}{(2n)! \cdot (n - 1)! \cdot (n - 1)!}
    =
    \frac{2 \cdot n \cdot n}{2n \cdot (2n - 1)}
    =
    \frac{n}{2n - 1}
\]

\begin{exercise}{}{}
    Проделайте то же самое для нечетного числа вершин:

    Если в графе \( 2n + 1 \) вершина, то есть разрез величины \( \frac{|E| (n + 1)}{2n + 1} \)
\end{exercise}

\subsection{Нижняя оценка числа Рамсея}

\begin{lemma}{}{}
    Пусть \( A_1, \ldots, A_n \) --- события.

    Тогда \( P(A_1 \cup \ldots \cup A_n) \leq \sum P(A_i) \)
\end{lemma}

\begin{remark}{}{}
    \[
        R(n, k) \leq \binom{n + k - 2}{n - 1}
    \]
\end{remark}

\begin{coroll}{}{}
    \[
        R(k, k) \leq \binom{2k - 2}{k - 1} < 2^{2k - 2}
    \]
\end{coroll}

\begin{thrm}{}{}
    \[
        R(k, k) > \left[ \frac{k \cdot 2^{k / 2}}{2e}\right], \quad k \geq 3
    \]
\end{thrm}

Хотим построить граф на \( n = \left[ \frac{k \cdot 2^{k / 2}}{2e} \right] \) вершинах
без клик и независимых множеств размера \( k \).

\( \Omega \) --- все графы на \( n \) вершинах равновероятно,
\(
    \displaystyle
    | \Omega | = 2^{\binom{n}{2}}
\)

Для \( W \subseteq \{ 1, 2, \ldots, n \} \) посчитаем вероятности событий:
\begin{enumerate}
    \item
        \( A_W \) = \( W \) --- клика
    \item
        \( B_W \) = \( W \) --- независимое множество
\end{enumerate}

\[
    P(A_W) = P(B_W) = \frac{2^{\binom{n}{2} - \binom{k}{2}}}{2^{\binom{n}{2}}}
\]

\begin{gather*}
    P(\text{в \( G \) есть клика или независимое множество размера \( k \)})
    =
    P \left( \bigcup\limits_W (A_W + B_W) \right)
    \leq
    \\
    \leq
    \sum\limits_W ( P(A_W) + P(B_W) )
    =
    \binom{n}{k} \cdot 2^{1 - \binom{k}{2}}
    \leq
    \left( \frac{ne}{k} \right)^k \cdot 2^{1 - \binom{k}{2}}
    \leq
    \\
    \left( \frac{k \cdot 2^{k / 2} \cdot e}{2e \cdot k} \right)^k \cdot 2^{1 - \binom{k}{2}}
    =
    \left( \frac{2^{k / 2}}{2} \right)^k \cdot 2^{1 - \binom{k}{2}}
    =
    2^{\frac{k^2}{2} - k + 1 - \frac{k(k - 1)}{2}}
    =
    2^{1 - \frac{k}{2}}
\end{gather*}

Что меньше единицы при \( k \geq 3 \)

Значит с какой-то ненулевой вероятностью нам может попасть граф, для которого число Рамсея удовлетворяет
оценке из теоремы, что и требовалось доказать.

\subsection{Теорема Эрдеша}

\begin{thrm}{}{}
    \( \forall{k} \in \NN \) существует граф \( G \) такой, что:
    \begin{enumerate}
        \item
            \( S(G) > k \) (нет циклов длины \( k \) и меньше)
        \item
            \( \mathcal{X} (G) > k \)
    \end{enumerate}
\end{thrm}

Модель Эрдеша-Реньи: каждое ребро проводится с вероятностью \( p \).

Положим \( p = \frac{\ln n}{n} \).

Давайте докажем, что \( P \left( \alpha(G) \geq \frac{n}{2k} \right) \) стремится к нулю.

Положим \( r = \lceil \frac{n}{2k} \rceil \).

\begin{gather*}
    P \left( \alpha(G) \geq \frac{n}{2k} \right)
    =
    P \left( \bigcup\limits_{\substack{X \subseteq V  \\ |X| = r}} (X \text{ --- антиклика}) \right)
    \leq
    \\
    \leq
    \sum\limits_{\substack{X \subseteq V  \\ |X| = r}} P \left( X \text{ --- антиклика} \right)
    =
    \binom{n}{r} (1 - p)^{\binom{r}{2}}
\end{gather*}

Воспользуемся тем, что \( \binom{n}{r} \leq \left( \frac{ne}{r} \right)^r \) и \( e^{-p} \geq 1 - p \):
\[
    \binom{n}{r} (1 - p)^{\binom{r}{2}}
    \leq
    \left( \frac{ne}{r} \right)^r \cdot e^{-p \cdot \frac{r(r - 1)}{2}}
    =
    \left( \frac{ne}{r} \cdot e^{-p \cdot \frac{r - 1}{2}} \right)^r
\]

Поскольку \( r \to \infty \) при \( n \to \infty \),
достаточно показать, что выражение внутри скобок стремится к нулю:
\[
    \frac{ne}{r} \cdot e^{-p \cdot \frac{r - 1}{2}}
    \leq
    2ke \cdot n^{-\frac{r - 1}{2n}}
    \leq 
    2ke \cdot n^{-\frac{n}{2k \cdot 2n}}
    \leq
    2ke \cdot n^{-\frac{1}{4k}}
\]

Очевидно, что последнее выражение стремится к нулю.

Теперь покажем, что вероятность того, что у нас графе более \( \frac{n}{2} \) циклов длины \( \leq k \) тоже стремится к нулю.

По неравенству Маркова:
\(
    P \left( \text{\# плохих циклов \( \geq \frac{n}{2} \)} \right) \leq \frac{2 \cdot E(\text{\# число плохих циклов})}{n}
\)

Давайте оценим нужное нам математическое ожидание.

Сколько существует циклов на \( i \geq 3 \) вершинах?

Ответ: \( \frac{i!}{2i} \)

Используя это, оценим математическое ожидание:
\begin{gather*}
    E(\text{\# число плохих циклов})
    \leq
    \sum\limits_{i = 3}^{k} \binom{n}{i} \cdot \frac{i!}{2i} \cdot p^i
    \leq
    \\
    \leq
    \sum\limits_{i = 3}^{k} n \cdot (n - 1) \cdot \ldots \cdot (n - i + 1) \cdot p^i
    \leq
    \sum\limits_{i = 3}^{k} (np)^i
    \leq
    \\
    \leq
    \frac{(np)^{k + 1} - 1}{np - 1}
    =
    \frac{(\ln n)^{k + 1} - 1}{\ln n - 1}
    <
    (\ln n)^{k + 1}
\end{gather*}

Подставим в неравенство Маркова выше:
\[
    P \left( \text{\# плохих циклов \( \geq \frac{n}{2} \)} \right) \leq \frac{2 (\ln n)^{k + 1}}{n}
\]

Выражение справа стремится к нулю --- доказали.

Выберем \( n \) такое, что:
\[
    \begin{cases}
        P \left( \alpha(G) \geq \frac{n}{2k} \right) < \frac{1}{2}
        \\
        P(\text{\# плохих циклов \( \geq \frac{n}{2} \)}) < \frac{1}{2}
    \end{cases}
\]

Тогда найдется граф такой, что \( \alpha(G) < \frac{n}{2k} \), и в \( G \) не более \( \frac{n}{2} \)
циклов длины не больше \( k \). Давайте выкинем из каждого такого цикла по вершине.
Тогда в \( \tilde{G} \) не менее \( \frac{n}{2} \) вершин, в нем нет циклов длины меньше \( k \),
а также \( \alpha(\tilde{G}) < \frac{n}{2k} \).

Но тогда \( \mathcal{X}(\tilde{G}) \geq \frac{n}{2 \alpha(\tilde{G})} > \frac{n}{2 \cdot \frac{n}{2k}} = k \).

То есть граф \( \tilde{G} \) является искомым для заданного \( k \) --- доказали.

% План доказательства:
% \begin{enumerate}
%     \item
%         Используем модель Эрдеша-Реньи случайных графов \( G(n, p), p \in [0, 1] \):
%         каждое ребро проводится с вероятностью \( p \)
%     \item
%         Воспользуемся тем, что \( \mathcal{X}(G) \geq \frac{|V|}{\alpha(G)} \)
% \end{enumerate}
